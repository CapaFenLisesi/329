\chapter{Monte-Carlo Methods}
\section{Introduction}
Numerical methods which make use of random numbers are called {\em Monte-Carlo methods}---after the famous
casino. The obvious applications of such methods are in {\em stochastic physics}: {\em e.g.}, statistical
thermodynamics. However, there are other, less obvious, applications: {\em e.g.}, the evaluation of
multi-dimensional integrals.

\section{Random Numbers}
No numerical algorithm can generate a truly random sequence of numbers, However, there exist
algorithms which generate repeating sequences of $M$ (say) integers which are, to 
a fairly good approximation, randomly distributed in the range
$0$ to $M-1$. Here, $M$ is a (hopefully) large integer. This type of
sequence is termed {\em psuedo-random}. 

The most well-known algorithm for generating psuedo-random sequences 
of integers is the so-called {\em linear congruental} method.
The formula linking the $n$th and $(n+1)$th integers in the sequence is
\begin{equation}\label{e71}
I_{n+1} = (A\,I_n+C)\,{\rm mod}\,M,
\end{equation}
where $A$, $C$, and $M$ are positive integer constants. 
The first number in the sequence, the so-called ``seed'' value, is selected by the user.

Consider an example case in which $A=7$, $C=0$, and $M=10$. A typical sequence of numbers generated by
formula~(\ref{e71}) is
\begin{equation}
I=\{3, 1, 7, 9, 3, 1, \cdots\}.
\end{equation}
Evidently, the above choice of values for $A$, $C$, and $M$ is not a particularly good one, since the
sequence repeats after only four iterations. However, if  $A$, $C$, and $M$ are properly
chosen then the sequence is of maximal length ({\em i.e.}, of length $M$), and approximately randomly
distributed in the range $0$ to $M-1$. 

The function listed below is an implementation of the linear congruental method.
{\small\begin{verbatim}
// random.cpp

// Linear congruential psuedo-random number generator.
// Generates psuedo-random sequence of integers in
//  range 0 .. RANDMAX. 

#define RANDMAX 6074  // RANDMAX = M - 1

int random (int seed = 0)
{
  static int next = 1;
  static int A = 106;
  static int C = 1283;
  static int M = 6075;

  if (seed) next = seed;
  next = next * A + C;
  return next % M;
}
\end{verbatim}}
The keyword {\tt static} in front of a local variable declaration indicates that the 
program should preserve the value of
that variable between function calls. In other words, if the static variable {\tt next}
has the value 999 on exit from function {\tt random} then the next time this function is
called {\tt next} will have exactly the same value. Note that the values of non-static
local variables are not preserved between function calls. The {\tt = 0} in the first line
of function {\tt random} is a default value for the argument {\tt seed}. In fact, {\tt random}
can be called in one of two ways. Firstly, {\tt random} can be called with no argument: {\em i.e.},
{\tt random ()}: in which case, {\tt seed} is given the default value 0. Secondly, {\tt random}
can be called with an integer argument: {\em i.e.},
{\tt random (n)}: in which case, the value of {\tt seed} is set to {\tt n}. The first way of calling
{\tt random} just returns the next integer in the psuedo-random sequence. The
second way seeds the sequence with the value {\tt n} ({\em i.e.}, $I_1$ is set to {\tt n}), and
then returns the next integer in the sequence ({\em i.e.}, $I_2$). Note that the function prototype for
{\tt random} takes the form {\tt int random (int = 0)}: the {\tt = 0} indicates
that the argument is optional.


The above function returns a pseudo-random integer in the range $0$ to {\tt RANDMAX} (where {\tt RANDMAX}
takes the value $M-1$). In order to obtain a random variable $x$, uniformly distributed in the
range 0 to 1, we would write
\begin{verbatim}
x = double (random ()) / double (RANDMAX);
\end{verbatim}
Now if $x$ is truly random then there should be no correlation between successive values of $x$. Thus,
a good way of testing our random number generator is to plot $x_j$ versus $x_{j+1}$ (where $x_j$ corresponds to
the $j$th number in the psuedo-random sequence) for many different values of $j$. For a good random number generator,
the plotted points should densely fill the unit square. Moreover, there should be no discernible pattern in the 
distribution of points.

\begin{figure}
\epsfysize=4in
\centerline{\epsffile{Chapter09/rand3.eps}}
\caption{\em Plot of $x_j$ versus $x_{j+1}$ for $j=1,10000$. Here, the $x_j$
are random values, uniformly distributed in the range 0 to 1, generated
using a linear congruental psuedo-random number generator characterized by
$A=106$, $C=1283$, and $M=6075$.}\label{rand3}
\end{figure}

Figure~\ref{rand3} shows a correlation plot for the first 10000 $x_j$--$x_{j+1}$ pairs generated
using a linear congruental  psuedo-random number generator characterized by
$A=106$, $C=1283$, and $M=6075$. It can be seen that this is a
poor choice of values for
$A$, $C$, and $M$, since the pseudo-random sequence repeats after a few iterations, yielding
$x_j$ values which do not densely fill the interval 0 to 1.

\begin{figure}
\epsfysize=4in
\centerline{\epsffile{Chapter09/rand4.eps}}
\caption{\em Plot of $x_j$ versus $x_{j+1}$ for $j=1,10000$. Here, the $x_j$
are random values, uniformly distributed in the range 0 to 1, generated
using a linear congruental psuedo-random number generator characterized by
$A=107$, $C=1283$, and $M=6075$.}\label{rand4}
\end{figure}

Figure~\ref{rand4} shows a correlation plot for the first 10000 $x_j$--$x_{j+1}$ pairs generated
using a linear congruental  psuedo-random number generator characterized by
$A=107$, $C=1283$, and $M=6075$. It can be seen that this is a
far better choice of values for
$A$, $C$, and $M$, since the pseudo-random sequence is of maximal length, yielding
$x_j$ values which are fairly evenly distributed in the range 0 to 1. However,
if we look carefully at Fig.~\ref{rand4}, we can see that there is a slight tendency for
the dots to line up in the horizontal and vertical directions. This indicates that the 
$x_j$ are not quite randomly distributed: {\em i.e.}, there is some correlation between
successive $x_j$ values. The problem is that $M$ is too low: {\em i.e.}, there is
not a sufficiently wide selection of different $x_j$ values in the interval 0 to 1.

\begin{figure}
\epsfysize=4in
\centerline{\epsffile{Chapter09/rand1.eps}}
\caption{\em Plot of $x_j$ versus $x_{j+1}$ for $j=1,10000$. Here, the $x_j$
are random values, uniformly distributed in the range 0 to 1, generated
using a linear congruental psuedo-random number generator characterized by
$A=1103515245$, $C=12345$, and $M=32768$.}\label{rand1}
\end{figure}

Figure~\ref{rand1} shows a correlation plot for the first 10000 $x_j$--$x_{j+1}$ pairs generated
using a linear congruental  psuedo-random number generator characterized by
$A=1103515245$, $C=12345$, and $M=32768$. The clumping of points in this figure indicates
that the $x_j$ are again not quite randomly distributed. This time the problem is integer overflow:
{\em i.e.}, the values of $A$ and $M$ are sufficiently large that $A\,I_n > 10^{32}-1$ for
many integers in the pseudo-random sequence. Thus, the algorithm (\ref{e71}) is not
being executed correctly.

Integer overflow can be overcome using {\em Schrange's algorithm}. If $y = (A\,z)\,{\rm mod} M$ then
\begin{equation}
y = \left\{\begin{array}{lcl}
A \,(z\,{\rm mod}\,q) - r \,(z/q) &\mbox{\hspace{1cm}}&\mbox{if $y > 0$}\\
A \,(z\,{\rm mod}\,q) - r \,(z/q) + M && \mbox{otherwise}
\end{array}
\right.,
\end{equation}
where $q = M/A$ and $r=M\verb!%!A$. The so-called {\em Park and Miller} method for generating a pseudo-random sequence
corresponds to a linear congruental method characterized by
the values $A=16807$, $C=0$, and $M=2147483647$. The function listed below implements
this method, using Schrange's algorithm to avoid integer overflow.
{\small\begin{verbatim}
// random.cpp

// Park and Miller's psuedo-random number generator.

#define RANDMAX 2147483646 // RANDMAX = M - 1

int random (int seed = 0)
{
  static int next = 1;
  static int A = 16807;
  static int M = 2147483647;   // 2^31 - 1
  static int q = 127773;       // M / A
  static int r = 2836;         // M % A

  if (seed) next = seed;
  next = A * (next % q) - r * (next / q);
  if (next < 0) next += M;
  return next;
}
\end{verbatim}}

\begin{figure}
\epsfysize=4in
\centerline{\epsffile{Chapter09/rand2.eps}}
\caption{\em Plot of $x_j$ versus $x_{j+1}$ for $j=1,10000$. Here, the $x_j$
are random values, uniformly distributed in the range 0 to 1, generated
using Park \& Miller's psuedo-random number generator.}\label{rand2}
\end{figure}

Figure~\ref{rand2} shows a correlation plot for the first 10000 $x_j$--$x_{j+1}$ pairs generated
using Park \& Miller's method.
We can now see no pattern whatsoever in the plotted points. This indicates that the $x_j$ are indeed
randomly distributed in the range 0 to 1. From now on, we shall use Park \& Miller's method
to generate all the psuedo-random numbers needed in our investigation of Monte-Carlo methods.

\section{Distribution Functions}
Let $P(x)\,dx$ represent the probability of finding the random variable $x$ in the
interval $x$ to $x+dx$. Here, $P(x)$ is termed a {\em probability density}. Note that
$P=0$ corresponds to no chance, whereas $P=1$ corresponds to certainty. Since
it is certain that the value of $x$ lies in the range $-\infty$ to $+\infty$,
probability densities are subject to the normalizing constraint
\begin{equation}
\int_{-\infty}^{+\infty} P(x)\,dx = 1.
\end{equation}

Suppose that we wish to construct a random variable $x$ which is uniformly
distributed in the range $x_1$ to $x_2$. In other words, the probability
density of $x$ is
\begin{equation}
P(x) = \left\{\begin{array}{lcl}
1/(x_2-x_1) &\mbox{\hspace{1cm}}&\mbox{if $x_1 \leq x \leq x_2$}\\
0&& \mbox{otherwise}
\end{array}
\right..
\end{equation}
Such a variable is constructed as follows
\begin{verbatim}
x = x1 + (x2 - x1) * double (random ()) / double (RANDMAX);
\end{verbatim}

There are two basic methods of constructing non-uniformly distributed random variables: 
{\em i.e.}, the {\em transformation method} and the {\em rejection method}. We shall examine
 each of these methods in turn.

Let us first consider the transformation method. Let $y=f(x)$, where $f$ is a known function, and
$x$ is a random variable. Suppose that
the probability density of $x$ is $P_x(x)$. What is the probability density, $P_y(y)$, of $y$?
Our basic rule is the conservation of probability:
\begin{equation}
|P_x(x)\,dx| = |P_y(y)\,dy|.
\end{equation}
In other words, the probability of finding $x$ in the interval $x$ to $x+dx$ is the
same as the probability of finding $y$ in the interval $y$ to $y+dy$. It follows
that
\begin{equation}
P_y(y) = \frac{P_x(x)}{|f'(x)|},
\end{equation}
where $f'=df/dx$.

For example, consider the {\em Poisson distribution}:
\begin{equation}
P_y(y) = \left\{\begin{array}{lcl}
{\rm e}^{-y}&\mbox{\hspace{1cm}}&\mbox{if $0 \leq y \leq \infty$}\\
0&& \mbox{otherwise}
\end{array}
\right..
\end{equation}
Let $y=f(x)=-\ln x$, so that $|f'| = 1/x$. Suppose that
\begin{equation}
P_x(x) = \left\{\begin{array}{lcl}
1&\mbox{\hspace{1cm}}&\mbox{if $0 \leq x \leq 1$}\\
0&& \mbox{otherwise}
\end{array}
\right..
\end{equation}
It follows that
\begin{equation}
P_y(y) = \frac{1}{|f'|}= x = {\rm e}^{-y},
\end{equation}
with $x=0$ corresponding to $y=\infty$, and $x=1$ corresponding to $y=0$. We conclude that
if
{\small\begin{verbatim}
x = double (random ()) / double (RANDMAX);
y = - log (x);
\end{verbatim}}
\noindent then {\tt y} is distributed according to the Poisson distribution.

The transformation method requires a differentiable probability distribution function. This is
not always practical. In such cases, we can use the rejection method instead.

Suppose that we desire a random variable $y$ distributed with density $P_y(y)$ in the
range $y_{\rm min}$ to $y_{\rm max}$. Let $P_{y\,{\rm max}}$ be the maximum value of $P(y)$ in this range (see
Fig.~\ref{rej}).
The rejection method is as follows. The variable $y$ is sampled randomly in the range $y_{\rm min}$
 to $y_{\rm max}$. 
For each value of $y$ we first evaluate $P_y(y)$. We next generate a random number $x$ which is
uniformly distributed in the range 0 to $P_{y\,{\rm max}}$. Finally, if $P_y(y) < x$ then we reject
the $y$ value; otherwise, we keep it. If this prescription is followed then $y$ will
be distributed according to $P_y(y)$.

\begin{figure}
\epsfysize=2.5in
\centerline{\epsffile{Chapter09/rejection.eps}}
\caption{\em The rejection method.}\label{rej}
\end{figure}

As an example, consider the Gaussian distribution:
\begin{equation}
P_y(y) =\frac{{\rm exp}[(y-\bar{y})^2/2\,\sigma^2]}{\sqrt{2\pi}\,\sigma},
\end{equation}
where $\bar{y}$ is the mean value of $y$, and $\sigma$ is the standard deviation.
Let
\begin{eqnarray}
y_{\rm min} &=& \bar{y}-4\,\sigma,\\[0.5ex]
y_{\rm max} &=& \bar{y}+4\,\sigma,
\end{eqnarray}
since there is a negligible chance that $y$ lies more than 4 standard deviations
from its mean value.
It follows that
\begin{equation}
P_{y\,{\rm max}} = \frac{1}{\sqrt{2\pi}\,\sigma},
\end{equation}
with the maximum occurring at $y=\bar{y}$.
The function listed below employs the rejection method to return a
random variable distributed according to a Gaussian distribution
with mean {\tt mean} and standard deviation {\tt sigma}:
{\small\begin{verbatim}
// gaussian.cpp

// Function to return random variable distributed
// according to Gaussian distribution with mean mean
// and standard deviation sigma.

#define RANDMAX 2147483646

int random (int = 0);

double gaussian (double mean, double sigma)
{
  double ymin = mean - 4. * sigma;
  double ymax = mean + 4. * sigma;
  double Pymax = 1. / sqrt (2. * M_PI) / sigma;

  // Calculate random value uniformly distributed 
  //  in range ymin to ymax
  double y = ymin + (ymax - ymin) * double (random ()) / double (RANDMAX);

  // Calculate Py
  double Py = exp (- (y - mean) * (y - mean) / 2. / sigma / sigma) /
    sqrt (2. * M_PI) / sigma;

  // Calculate random value uniformly distributed in range 0 to Pymax
  double x = Pymax * double (random ()) / double (RANDMAX);

  // If x > Py reject value and recalculate
  if (x > Py) return gaussian (mean, sigma);
  else return y;
}
\end{verbatim}}
\noindent Figure~\ref{rej1} illustrates the performance of the above function. It can be seen
that the function successfully returns a random value distributed according to the Gaussian distribution.

\begin{figure}
\epsfysize=3in
\centerline{\epsffile{Chapter09/reject.eps}}
\caption{\em A million values returned by function {\tt gaussian} with
{\tt mean = 5.}\@ and {\tt sigma = 1.25}. The values are binned
in 100 bins of width 0.1. The figure shows the number of points
in each bin divided by a suitable normalization factor. A Gaussian curve
is shown for comparison.}\label{rej1}
\end{figure}

\section{Monte-Carlo Integration}
Consider a one-dimensional integral: $\int_{x_l}^{x_h} f(x)\,dx$. We can evaluate this
integral numerically by dividing the interval $x_l$ to $x_h$ into $N$ identical subdivisions of
width
\begin{equation}
h = \frac{x_h-x_l}{N}.
\end{equation}
Let $x_i$ be the {\em midpoint} of the $i$th subdivision, and let $f_i = f(x_i)$.
Our approximation to the integral takes the form
\begin{equation}
\int_{x_l}^{x_h} f(x)\,dx\simeq \sum_{i=1}^{N} f_i\,h
\end{equation}
This integration method---which is known as the {\em midpoint method}---is not particularly
accurate, but is very easy to generalize to multi-dimensional integrals. 

	What is the  error associated with the midpoint method? Well, the error is the
product of the error per subdivision, which is $O(h^2)$, and the number of subdivisions,
which is $O(h^{-1})$. The error per subdivision follows from the linear variation
of $f(x)$ within each subdivision. Thus, the overall error is $O(h^2)\times O(h^{-1}) = 
O(h)$. Since, $h\propto N^{-1}$, we can write
\begin{equation}
\int_{x_l}^{x_h} f(x)\,dx\simeq \sum_{i=1}^{N} f_i\,h + O(N^{-1}).
\end{equation}

Let us now consider a two-dimensional integral. For instance, the area enclosed by a curve.
We can evaluate such an integral by dividing space into identical squares of dimension $h$,
and then counting the number of squares, $N$ (say), whose midpoints lie within the curve.
Our approximation to the integral  then takes the form
\begin{equation}
A \simeq N\,h^2.
\end{equation}
This is the two-dimensional generalization of the midpoint method.

What is the error associated with the midpoint method in two-dimensions? Well, the error
is generated by those squares which are intersected by the curve. These squares either
contribute wholly or not at all to the integral, depending on whether their midpoints
lie within the curve. In reality, only those parts of the intersected squares which lie
within the curve should contribute to the integral. Thus, the error is the product of
the area of a given square, which is $O(h^2)$, and the number of squares intersected
by the curve, which is $O(h^{-1})$. Hence, the overall error is $O(h^2)\times O(h^{-1}) = 
O(h)= O(N^{-1/2})$. It follows that we can write
\begin{equation}
A = N\,h^2 + O(N^{-1/2}).
\end{equation}

Let us now consider a three-dimensional integral. For instance, the volume enclosed by a surface.
We can evaluate such an integral by dividing space into identical cubes of dimension $h$,
and then counting the number of cubes, $N$ (say), whose midpoints lie within the surface.
Our approximation to the integral  then takes the form
\begin{equation}
V \simeq N\,h^3.
\end{equation}
This is the three-dimensional generalization of the midpoint method.

What is the error associated with the midpoint method in three-dimensions? Well, the error
is generated by those cubes which are intersected by the surface. These cubes either
contribute wholly or not at all to the integral, depending on whether their midpoints
lie within the surface. In reality, only those parts of the intersected cubes which lie
within the surface should contribute to the integral. Thus, the error is the product of
the volume of a given cube, which is $O(h^3)$, and the number of cubes intersected
by the surface, which is $O(h^{-2})$. Hence, the overall error is $O(h^3)\times O(h^{-2}) = 
O(h)= O(N^{-1/3})$. It follows that we can write
\begin{equation}
V = N\,h^3 + O(N^{-1/3}).
\end{equation}

Let us, finally, consider using the midpoint method to evaluate the volume, $V$, of a $d$-dimensional
hypervolume enclosed by a $(d-1)$-dimensional hypersurface. It is clear, from the above examples,
that
\begin{equation}
V = N\,h^d + O(N^{-1/d}),
\end{equation}
where $N$ is the number of identical hypercubes into which the hypervolume is divided.
Note the increasingly slow fall-off of the error with $N$ as the dimensionality, $d$,
becomes greater. The explanation for this phenomenon is quite simple. Suppose that $N=10^6$.
With $N=10^6$  we can divide a unit line into (identical) subdivisions whose linear extent
is $10^{-6}$, but we can only divide a unit  area into subdivisions whose linear extent
is $10^{-3}$, and a   unit volume into subdivisions whose linear extent
is $10^{-2}$. Thus, for a fixed number of subdivisions the grid spacing (and, hence, the
integration error) increases dramatically
with increasing dimension.

Let us now consider the so-called {\em Monte-Carlo method} for evaluating multi-dimensional
integrals. Consider, for example, the evaluation of the area, $A$, enclosed by a curve, $C$.
Suppose that the curve  lies wholly within some simple domain of area $A'$, as
illustrated in Fig.~\ref{fmc}. Let us generate $N'$ points which are {\em randomly} distributed
throughout $A'$. Suppose that $N$ of these points lie within curve $C$. Our estimate for the area enclosed
by the curve is simply
\begin{equation}\label{emc}
A = \frac{N}{N'}\,A'.
\end{equation}

\begin{figure}
\epsfysize=2in
\centerline{\epsffile{Chapter09/mc.eps}}
\caption{\em The Monte-Carlo integration method.}\label{fmc}
\end{figure}

What is the error associated with the Monte-Carlo integration method? Well, each
point has a probability $p=A/A'$ of lying within the curve. Hence, the determination
of whether a given point lies within the curve is like the measurement of a
random variable $x$ which has two possible values: 1 (corresponding to the point being inside the curve)
with probability $p$, and 0 (corresponding to the point being outside the curve) with probability
$1-p$. If we make $N'$ measurements of $x$ ({\em i.e.}, if we scatter $N'$ points
throughout $A'$) then the number of points lying within the curve is
\begin{equation}
N = \sum_{i=1,N'} x_i,
\end{equation}
where $x_i$ denotes the $i$th measurement of $x$. 
Now, the mean value of $N$ is
\begin{equation}
\bar{N} = \sum_{i=1,N'} \bar{x}= N'\,\bar{x},
\end{equation}
where 
\begin{equation}
\bar{x} = 1\times p + 0\times (1-p) = p.
\end{equation}
Hence,
\begin{equation}
\bar{N} = N'\,p = N'\,\frac{A}{A'},
\end{equation}
which is consistent with Eq.~(\ref{emc}). We conclude that, on average, a measurement of $N$
leads to the correct answer. 
But, what is the scatter in such a measurement? Well, if $\sigma$ represents
the {\em standard deviation} of $N$ then we have
\begin{equation}
\sigma^2 = \overline{(N-\bar{N})^2},
\end{equation}
which can also be written
\begin{equation}
\sigma^2 = \sum_{i,j,=1,N'} \overline{(x_i-\bar{x})(x_j-\bar{x})}.
\end{equation}
However, $\overline{(x_i-\bar{x})(x_j-\bar{x})}$ equals $\overline{(x-\bar{x})^2}$ if $i=j$, and
equals zero, otherwise, since successive measurements of $x$ are {\em uncorrelated}. Hence,
\begin{equation}
\sigma^2 = N'\,\overline{(x-\bar{x})^2}.
\end{equation}
Now,
\begin{equation}
\overline{(x-\bar{x})^2}=\overline{(x^2-2\,x\,\bar{x}+\bar{x}^2)} = \overline{x^2} - \bar{x}^2,
\end{equation}
and
\begin{equation}
\overline{x^2} = 1^2\times p + 0^2\times (1-p) = p.
\end{equation}
Thus,
\begin{equation}
\overline{(x-\bar{x})^2}= p -p^2 =p\,(1-p),
\end{equation}
giving
\begin{equation}
\sigma = \sqrt{N'\,p\,(1-p)}.
\end{equation}
Finally, since the likely values of $N$ lie
in the range $N=\bar{N}\pm\sigma$, we can write
\begin{equation}
N = N'\,\frac{A}{A'} \pm \sqrt{N'\,\frac{A}{A'}\left(1-\frac{A}{A'}\right)}.
\end{equation}
It follows from Eq.~(\ref{emc}) that
\begin{equation}\label{e999}
A = A'\,\frac{N}{N'} \pm \frac{\sqrt{A\,(A'-A)}}{\sqrt{N'}}.
\end{equation}
In other words, the error scales like $(N')^{-1/2}$. 

The Monte-Carlo method generalizes immediately to $d$-dimensions.
For instance, consider a $d$-dimensional hypervolume $V$ enclosed by a
$(d-1)$-dimensional hypersurface $A$. Suppose that $A$ lies wholly
within some simple hypervolume $V'$. We can generate $N'$ points randomly
distributed throughout $V'$. Let $N$ be the number of these
points which lie within $A$. It follows that our estimate for $V$
is simply
\begin{equation}
V = \frac{N}{N'}\,V'.
\end{equation}
Now, there is nothing in our derivation of Eq.~(\ref{e999}) which depends on the
fact that the integral in question is two-dimensional. Hence, we can generalize
this equation to give
\begin{equation}
V = V'\,\frac{N}{N'} \pm \frac{\sqrt{V\,(V'-V)}}{\sqrt{N'}}.
\end{equation}
We conclude that the error associated with Monte-Carlo integration {\em always}
scales like $(N')^{-1/2}$, irrespective of the dimensionality of the integral.

We are now in a position to compare and contrast the midpoint and Monte-Carlo
methods for evaluating multi-dimensional integrals.
In the midpoint method, we fill space with an {\em evenly spaced} mesh of $N$ (say) points
({\em i.e.}, the midpoints of the subdivisions), and
the overall error scales like $N^{-1/d}$, where $d$ is the dimensionality of the integral.
In the Monte-Carlo method, we fill space with $N$ (say) {\em randomly distributed}
points, and the overall error scales like $N^{-1/2}$, irrespective of the
dimensionality of the integral. For a one-dimensional integral ($d=1$), the
midpoint method is {\em more efficient} than the Monte-Carlo method, since in the
former case the error scales like $N^{-1}$, whereas in the latter  the
error scales like $N^{-1/2}$.  For a two-dimensional integral ($d=2$),
the midpoint and Monte-Carlo methods are both {\em equally efficient}, since in
both cases the error scales like  $N^{-1/2}$. Finally, for
a three-dimensional integral ($d=3$), the
midpoint method is {\em less efficient} than the Monte-Carlo method, since in the
former case the error scales like $N^{-1/3}$, whereas in the latter  the
error scales like $N^{-1/2}$. We conclude that for a sufficiently high dimension
integral the Monte-Carlo method is always going to be more efficient than an
integration method (such as the midpoint method) which relies on a uniform grid.

\begin{figure}
\epsfysize=2in
\centerline{\epsffile{Chapter09/exam.eps}}
\caption{\em Example calculation: volume of unit-radius 2-dimensional sphere enclosed
in a close-fitting 2-dimensional cube.}\label{exam}
\end{figure}

Up to now, we have only considered how the Monte-Carlo method can be employed to
evaluate a rather special class of integrals in which the integrand function
can only take the values 0 or 1. However, the Monte-Carlo method can easily be
adapted to evaluate more general integrals.  Suppose that we
wish to evaluate $\int f\,dV$, where $f$ is a general function and the
domain of integration is of arbitrary dimension. We proceed by randomly scattering
$N$ points throughout the integration domain and  calculating $f$ at each point.
Let $x_i$ denote the $i$th point. The Monte-Carlo approximation to the integral
is simply
\begin{equation}
\int f\,dV = \frac{1}{N}\sum_{i=1,N} f(x_i) + O\left(\frac{1}{\sqrt{N}}\right).
\end{equation}

\begin{figure}
\epsfysize=3in
\centerline{\epsffile{Chapter09/midpoint.eps}}
\caption{\em The integration error, $\epsilon$, versus the number of grid-points, $N$, for three
integrals evaluated using the midpoint method. The integrals are the
area of a unit-radius circle (solid curve), the volume of a unit-radius
sphere (dotted curve), and the volume of a unit-radius 4-sphere (dashed curve).}\label{midpoint}
\end{figure}

We end this section with an example calculation. Let us evaluate the volume of a unit-radius $d$-dimensional sphere, where $d$
runs from 2 to 4, using both the midpoint and Monte-Carlo methods. For
both methods, the domain of integration is a  cube, centred on the sphere, which is
such that the sphere just touches each face of the cube, as illustrated in Fig.~\ref{exam}.
\begin{figure}
\epsfysize=3in
\centerline{\epsffile{Chapter09/montecarlo.eps}}
\caption{\em The integration error, $\epsilon$, versus the number of points, $N$, for three
integrals evaluated using the Monte-Carlo method. The integrals are the
area of a unit-radius circle (solid curve), the volume of a unit-radius
sphere (dotted curve), and the volume of a unit-radius 4-sphere (dashed curve).}\label{montecarlo}
\end{figure}

Figure~\ref{midpoint} shows the integration error associated with the midpoint method
as a function of the number of grid-points, $N$. It can be seen that as the dimensionality
of the integral increases the error falls off much less rapidly as $N$ increases.

Figure~\ref{montecarlo} shows the integration error associated with the Monte-Carlo method
as a function of the number of points, $N$. It can be seen that there is very little
change in the rate at which the error falls off with increasing $N$
as the dimensionality of the integral varies. Hence, as the dimensionality, $d$, increases the Monte-Carlo method
eventually wins out over the midpoint method.

\section{The Ising Model}
{\em Ferromagnetism} arises when a collection of atomic spins align such that
their associated magnetic moments all point in the same direction, yielding a
net magnetic moment which is macroscopic in size. The simplest theoretical
description of ferromagnetism is called the {\em Ising model}. This model
was invented by Wilhelm Lenz in 1920: it is named after
Ernst Ising, a student of Lenz who chose the model as the subject of his
doctoral dissertation in 1925.

Consider $N$ atoms in the presence of a $z$-directed magnetic field of
strength $H$. Suppose that all
atoms are identical spin-$1/2$ systems. It follows that either $s_i=+1$
(spin up) or $s_i=-1$ (spin down), where $s_i$ is (twice) the $z$-component
of the $i$th atomic spin. The total energy of the system is
written:
\begin{equation}\label{eising}
E = - J\sum_{<i j>}s_i\,s_j -\mu\,H\sum_{i=1,N}s_i.
\end{equation}
Here, $<i j>$ refers to a sum over {\em nearest neighbour} pairs of atoms.
Furthermore, $J$ is called the {\em exchange energy}, whereas $\mu$ is the
atomic {\em magnetic moment}. Equation~(\ref{eising}) is the essence of the
Ising model.

The physics of the Ising model is as follows. The first term on the right-hand
side of Eq.~(\ref{eising}) shows that the overall energy is lowered when
neighbouring atomic spins are aligned. This effect is mostly
due to the {\em Pauli exclusion principle}. Electrons cannot occupy the same quantum
state, so two electrons on neighbouring atoms which have parallel spins
({\em i.e.}, occupy the same orbital state) cannot come close together in
space. No such restriction applies if the electrons have anti-parallel
spins. Different spatial separations imply different electrostatic
interaction energies, and the exchange energy, $J$, measures this difference.
Note that since the exchange energy is {\em electrostatic} in origin, it
can be quite large: {\em i.e.}, $J\sim 1$\,eV. This is far larger than the
energy associated with the direct magnetic interaction between neighbouring
atomic spins, which
is only about $10^{-4}$\,eV. However, the exchange effect is very short-range; hence,
the restriction to nearest neighbour interaction is quite realistic.

Our first attempt to analyze the Ising model will employ a simplification
known as the {\em mean field approximation}. The energy of the $i$th atom is written
\begin{equation}
e_i = -\frac{J}{2}\sum_{k=1,z}s_k\,s_i -\mu\,H\,s_i,
\end{equation}
where  the sum is over the $z$ nearest neighbours of atom $i$. The factor $1/2$
is needed to ensure that when we sum to obtain the total energy,
\begin{equation}
E =\sum_{i=1,N} e_i,
\end{equation}
we do not count each pair of neighbouring atoms twice.

We can write
\begin{equation}
e_i = -\mu\,H_{\rm eff}\,s_i,
\end{equation}
where
\begin{equation}
H_{\rm eff} = H + \frac{J}{2\,\mu}\sum_{k=1,z}s_k.
\end{equation}
Here, $H_{\rm eff}$ is the {\em effective magnetic field}, which is
made up of two components: the external field, $H$, and the internal
field generated by neighbouring atoms.

Consider a single atom in a magnetic field $H_m$. Suppose that
the atom is in thermal equilibrium with a heat bath of temperature
$T$. According to the well-known Boltzmann distribution, the mean spin
of the atom is
\begin{equation}
\bar{s } = \frac{{\rm e}^{+\beta\,\mu\,H_m} - {\rm e}^{-\beta\,\mu\,H_m}}
{{\rm e}^{+\beta\,\mu\,H_m} + {\rm e}^{-\beta\,\mu\,H_m}},
\end{equation}
where $\beta = 1/kT$, and $k$ is the Boltzmann constant. The above expression follows because the energy of
the ``spin up'' state ($s=+1$) is $-\mu\,H_m$, whereas the energy of
the ``spin down'' state ($s=-1$) is $+\mu\,H_m$. Hence,
\begin{equation}\label{emf1}
\bar{s} = \tanh(\beta\,\mu\,H_m).
\end{equation}

Let us assume that all atoms have {\em identical} spins: {\em i.e.}, $s_i=\bar{s}$. 
This assumption is known as the ``mean field approximation''.
We can write
\begin{equation}\label{emf2}
H_{\rm eff} = H + \frac{z\,J\,\bar{s}}{2\,\mu}.
\end{equation}
Finally, we can combine Eqs.~(\ref{emf1}) and (\ref{emf2}) (identifying  $H_m$ and $H_{\rm eff}$)
to obtain
\begin{equation}\label{emf3}
\bar{s} = \tanh\left\{\beta\,\mu\,H + \beta\,z\,J\,\bar{s}/2\right\}.
\end{equation}
Note that  the heat bath in which a given atom is immersed is simply the
rest of the atoms. Hence, $T$ is the temperature of the atomic array.
It is helpful to define the {\em critical temperature}, 
\begin{equation}
T_c = \frac{z\,J}{2\,k},
\end{equation}
and the {\em critical magnetic field},
\begin{equation}
H_c = \frac{k\,T_c}{\mu} = \frac{z\,J}{2\,\mu}.
\end{equation}
Equation~(\ref{emf3}) reduces to
\begin{equation}
\bar{s} = \tanh\left\{\frac{T_c}{T}\left(\frac{H}{H_c} + \bar{s}\right)\right\}.
\end{equation}
The above equation cannot be solved analytically. However, it is fairly
easily to solve numerically using the following iteration scheme:
\begin{equation}\label{eiter}
\bar{s}_{i+1} = \tanh\left\{\frac{T_c}{T}\left(\frac{H}{H_c} + \bar{s}_i\right)\right\}.
\end{equation}
The above formula is iterated until $\bar{s}_{i+1}\rightarrow \bar{s}_i$. 

It is helpful to define the {\em net magnetization},
\begin{equation}
M=\mu\sum_{i=1,N} s_i= \mu\,N\,\bar{s},
\end{equation}
the net  energy,
\begin{equation}
E=\sum_{i=1,N} e_i= -N\,k\,T_c\left(\frac{H}{H_c}+\bar{s}\right)\bar{s},
\end{equation}
and the {\em heat capacity},
\begin{equation}
C=\frac{dE}{dT}.
\end{equation}

\begin{figure}
\epsfysize=3in
\centerline{\epsffile{Chapter09/ising2.eps}}
\caption{\em  The net magnetization, $M$, of a collection of $N$ ferromagnetic atoms as a
function of the temperature, $T$, in the absence of an external magnetic
field. Calculation performed using the mean field approximation.}\label{ising2}
\end{figure}

Figures~\ref{ising1}, \ref{ising2}, and \ref{ising3} show the net magnetization, net energy,
and heat capacity calculated from the iteration formula (\ref{eiter}) in the
absence of an external magnetic field ({\em i.e.}, with $H=0$). It
can be seen that below the critical (or ``Curie'') temperature, $T_c$, there is
{\em spontaneous magnetization}: {\em i.e.}, the exchange effect is sufficiently large
to cause neighbouring atomic spins to spontaneously align. On the other hand, 
thermal fluctuations completely eliminate any alignment above the critical temperature. Moreover, at the
critical temperature there is a
{\em discontinuity} in the first derivative of the energy, $E$, with respect to
the temperature, $T$. This discontinuity generates a downward jump
in the heat capacity, $C$, at $T=T_c$. The sudden loss of spontaneous
magnetization as the temperature exceeds the critical temperature is a type of
 {\em phase transition}.

\begin{figure}
\epsfysize=3in
\centerline{\epsffile{Chapter09/ising1.eps}}
\caption{\em The net energy, $E$, of a collection of $N$ ferromagnetic atoms as a
function of the temperature, $T$, in the absence of an external magnetic
field. Calculation performed using the  mean field approximation.}\label{ising1}
\end{figure}

Now, according to the conventional classification of {\em phase transitions}, a
transition is {\em first-order} if the energy is discontinuous with respect
to the order parameter ({\em i.e.}, in this case, the temperature), and {\em second-order}
if the energy is continuous, but its first derivative with respect to the
order parameter is discontinuous, {\em etc.} We conclude that the loss of
spontaneous magnetization in a ferromagnetic material as the temperature
exceeds the critical temperature  is a second-order phase transition.

\begin{figure}
\epsfysize=3in
\centerline{\epsffile{Chapter09/ising3.eps}}
\caption{\em  The heat capacity, $C$, of a collection of $N$ ferromagnetic atoms as a
function of the temperature, $T$, in the absence of an external magnetic
field. Calculation performed using the mean field approximation.}\label{ising3}
\end{figure}

In order to see an example of a first-order phase transition, let us examine
the behaviour of the magnetization, $M$, as the external field, $H$, is
varied at constant temperature, $T$. 

\begin{figure}
\epsfysize=3in
\centerline{\epsffile{Chapter09/bs4.eps}}
\caption{\em  The net magnetization, $M$, of a collection of $N$ ferromagnetic atoms as a
function of the external magnetic field, $H$, at constant temperature, $T<T_c$.
 Calculation performed using the mean field approximation.}\label{bs4}
\end{figure}

\begin{figure}
\epsfysize=3in
\centerline{\epsffile{Chapter09/bs3.eps}}
\caption{\em The net energy, $E$, of a collection of $N$ ferromagnetic atoms as a
function of the external magnetic field, $H$, at constant temperature, $T<T_c$.
 Calculation performed using the mean field approximation.
}\label{bs3}
\end{figure}

Figures~\ref{bs4} and \ref{bs3} show the magnetization, $M$, and energy, $E$, versus
external field-strength, $H$, calculated from the iteration formula (\ref{eiter})
at some constant temperature, $T$, which is less
than the critical temperature, $T_c$. It can be seen that $E$ is
{\em discontinuous}, indicating the presence of a first-order phase transition.
Moreover, the system exhibits {\em hysteresis}---meta-stable
states exist within a certain range of $H$ values, and the magnetization of the system
at fixed $T$ and $H$ (within the aforementioned range) depends on its {\em past history}:
{\em i.e.}, on whether $H$ was increasing or decreasing when it entered the meta-stable
range.

\begin{figure}
\epsfysize=3in
\centerline{\epsffile{Chapter09/bs2.eps}}
\caption{\em  The net magnetization, $M$, of a collection of $N$ ferromagnetic atoms as a
function of the external magnetic field, $H$, at constant temperature, $T=T_c$.
 Calculation performed using the mean field approximation.}\label{bs2}
\end{figure}

\begin{figure}
\epsfysize=3in
\centerline{\epsffile{Chapter09/bs1.eps}}
\caption{\em  The net energy, $E$, of a collection of $N$ ferromagnetic atoms as a
function of the external magnetic field, $H$, at constant temperature, $T=T_c$.
 Calculation performed using the mean field approximation.}\label{bs1}
\end{figure}

Figures~\ref{bs2} and \ref{bs1} show the magnetization, $M$, and energy, $E$, versus
external field-strength, $H$, calculated from the iteration formula (\ref{eiter})
at a constant temperature, $T$, which is equal to the critical temperature, $T_c$.
 It can be seen that $E$ is now
{\em continuous}, and there are no meta-stable states. We conclude that  first-order
phase transitions and hysteresis only occur, as the external field-strength is varied, when the
temperature lies below the critical temperature: {\em i.e.}, when the ferromagnetic
material in question is capable of spontaneous magnetization.

The above calculations, which are based on the mean field approximation, correctly predict
the existence of first- and second-order phase transitions when $H\neq 0$ and $H=0$,
respectively. However, these calculations get some of the details of the second-order
phase transition  wrong. In order to do a better job, we must abandon the mean
field approximation and
adopt a Monte-Carlo approach.

\begin{figure}
\epsfysize=2in
\centerline{\epsffile{Chapter09/farray.eps}}
\caption{\em  A two-dimensional array of atoms.}\label{farray}
\end{figure}

Let us consider a two-dimensional square array of atoms. Let $L$ be the
size of the array, and $N=L^2$ the number of atoms in the array, as shown in
Fig.~\ref{farray}. The Monte-Carlo approach to the Ising model, which completely avoids
the use of the mean field approximation, is based on the following
algorithm:
\begin{itemize}
\item Step through each atom in the array in turn:
\begin{itemize}
\item For a given atom, evaluate the change in energy of the
system, ${\mit\Delta}E$, when the atomic spin is flipped.
\item If ${\mit\Delta}E<0$ then flip the spin.
\item If ${\mit\Delta}E>0$ then flip the spin with probability $P=\exp(-\beta\,{\mit\Delta}E)$. 
\end{itemize}
\item Repeat the process many times until thermal equilibrium is achieved.
\end{itemize}
The purpose of the algorithm is to shuffle through all  possible states of the
system, and to ensure that the system occupies a given state with the
Boltzmann probability: {\em i.e.}, with a probability proportional
to $\exp(-\beta\,E)$, where $E$ is the energy of the state.

In order to demonstrate that the above algorithm is correct, let us consider
flipping the spin of the $i$th atom. Suppose that this operation causes the
system to make a transition from state $a$ (energy, $E_a$) to state $b$ (energy, $E_b$).
Suppose, further, that $E_a < E_b$. According to the above algorithm, the probability
of a transition from state $a$ to state $b$ is
\begin{equation}
P_{a\rightarrow b} = \exp[-\beta\,(E_b-E_a)],
\end{equation}
whereas the probability of a transition from state $b$ to state $a$ is
\begin{equation}
P_{b\rightarrow a} = 1.
\end{equation}
In thermal equilibrium, the well-known {\em principal of detailed balance} implies that
\begin{equation}\label{edb}
P_a\,P_{a\rightarrow b} = P_b\,P_{b\rightarrow a},
\end{equation}
where $P_a$ is the probability that the system occupies state $a$, and  
 $P_b$ is the probability that the system occupies state $b$.
Equation~(\ref{edb}) simply states that in thermal equilibrium the rate at which
the system makes transitions from state $a$ to state $b$ is equal to the rate at
which the system makes  reverse transitions. The previous equation can be
rearranged to give
\begin{equation}
\frac{P_b}{P_a} = \exp[-\beta\,(E_b-E_a)],
\end{equation}
which is consistent with the Boltzmann distribution.

Now, each atom in our array has {\em four} nearest neighbours, except for  atoms on the
edge of the array, which have less than four   neighbours. We can eliminate
this annoying special behaviour by adopting {\em periodic boundary conditions}:
{\em i.e.}, by identifying opposite edges of the array. Indeed, we can think of the
array as existing on the surface of a torus.

It is helpful to define
\begin{equation}
T_0 = \frac{J}{k}.
\end{equation}
Now, according to mean field theory,
\begin{equation}
T_c = \frac{z\,J}{2\,k}= 2\,T_0.
\end{equation}
The evaluation of
\begin{equation}\label{direct}
C = \lim_{{\mit\Delta}T\rightarrow 0}\frac{{\mit\Delta}E}{{\mit\Delta} T}
\end{equation}
via the direct method is difficult due to  statistical noise in the energy,
$E$. Instead, we can make use of a standard result in equilibrium statistical
thermodynamics:
\begin{equation}\label{indirect}
C = \frac{\sigma_{E}^{\,2}}{k\,T^2},
\end{equation}
where $\sigma_E$ is the standard deviation of fluctuations in $E$. 
Fortunately, it is
fairly easy to evaluate $\sigma_E$: we can simply employ the standard deviation
in $E$ from step to step in our Monte-Carlo iteration scheme.

\begin{figure}
\epsfysize=3in
\centerline{\epsffile{Chapter09/ts5.eps}}
\caption{\em The net magnetization, $M$, of a  $5\times 5$ array of ferromagnetic atoms as a
function of the temperature, $T$, in the absence of an external magnetic
field. Monte-Carlo simulation.}\label{ff7}
\end{figure}

Figures~\ref{ff7}--\ref{ff10} show magnetization and heat capacity versus temperature
curves for $L=5$, 10, 20, and 40 in the absence of an
external magnetic field. In all cases, the Monte-Carlo simulation is iterated 5000 times,
and the first 1000 iterations are discarded when evaluating $\sigma_E$ (in order to
allow the system to attain thermal equilibrium). The two-dimensional array of atoms is
initialized in a fully aligned state for each different value of the temperature. Since there is
no external magnetic field, it is irrelevant whether the magnetization, M,  is
positive or negative. Hence,  $M$ is replaced by $|M|$ in all plots.

\begin{figure}
\epsfysize=3in
\centerline{\epsffile{Chapter09/cv5.eps}}
\caption{\em  The heat capacity, $C$, of a  $5\times 5$ array of ferromagnetic atoms as a
function of the temperature, $T$, in the absence of an external magnetic
field. Monte-Carlo simulation. The solid curve shows the heat capacity calculated
from Eq.~(\ref{direct}), whereas the dotted curve shows the heat capacity calculated
from Eq.~(\ref{indirect}).}\label{ff7a}
\end{figure}

Note that the $M$ versus $T$ curves generated by the Monte-Carlo simulations
look very much like those predicted by the
mean field model. The resemblance increases as the size, $L$, of the atomic
array increases. The major difference is the presence of a magnetization ``tail'' for $T>T_c$ in
the Monte-Carlo simulations: {\em i.e.}, in the Monte-Carlo simulations the spontaneous magnetization
does not collapse to zero once the critical temperature is exceeded---there is
a small lingering magnetization for $T>T_c$. 
The $C$ versus $T$ curves show the heat capacity calculated directly ({\em i.e.}, $C={\mit\Delta E}/{\mit\Delta T}$),
and via the identity $C=\sigma_E^{\,2}/k\,T^2$. The latter method of calculation is
clearly far superior, since it generates significantly less statistical noise. Note that the heat capacity
{\em peaks} at the critical temperature: {\em i.e.}, unlike the mean field model, $C$ is
not zero for $T>T_c$. This effect is due to the residual magnetization present when $T>T_c$. 

\begin{figure}
\epsfysize=3in
\centerline{\epsffile{Chapter09/ts10.eps}}
\caption{\em The net magnetization, $M$, of a  $10\times 10$ array of ferromagnetic atoms as a
function of the temperature, $T$, in the absence of an external magnetic
field. Monte-Carlo simulation.}
\end{figure}

Our best estimate for $T_c$ is obtained from the location of the peak in the $C$ versus $T$
curve in Fig.~\ref{ff10}. We obtain $T_c=2.27\,T_0$. Recall that the mean field model
yields $T_c = 2\,T_0$. The exact answer for a two-dimensional array of ferromagnetic atoms
is
\begin{equation}
T_c = \frac{2\,T_0}{\ln(1+\sqrt{2})} = 2.27\,T_0,
\end{equation}
which is consistent with our Monte-Carlo calculations.
The above analytic result was first obtained by Onsager in 1944.\footnote{L.~Onsager,
Phys.\ Rev.\ {\bf 65}, 117 (1944).}\@ Incidentally, Onsager's analytic
solution of the 2-D Ising model is one of the most complicated and involved calculations
in all of theoretical physics. Needless to say, no one has ever been able to find an
analytic solution of the Ising model in more than two dimensions.

\begin{figure}
\epsfysize=3in
\centerline{\epsffile{Chapter09/cv10.eps}}
\caption{\em The heat capacity, $C$, of a  $10\times 10$ array of ferromagnetic atoms as a
function of the temperature, $T$, in the absence of an external magnetic
field. Monte-Carlo simulation. The solid curve shows the heat capacity calculated
from Eq.~(\ref{direct}), whereas the dotted curve shows the heat capacity calculated
from Eq.~(\ref{indirect}).}\label{ff8}
\end{figure}

Note, from Figs.~\ref{ff7a}, \ref{ff8}, \ref{ff9}, and \ref{ff10},  that the height of the peak in the heat capacity curve at $T=T_c$ increases with increasing array
size, $L$. Indeed, a close examination of these figures yields
 $C_{\rm max}/N\,k = 0.95$ for $L=5$, $C_{\rm max}/N\,k = 1.34$ for $L=10$,
$C_{\rm max}/N\,k = 1.77$ for $L=20$, and $C_{\rm max}/N\,k = 2.16$ for $L=40$.
Figure~\ref{cvmax} shows $C_{\rm max}/N\,k$ plotted against $\ln L$ for $L=5$, 10, 20, and 40.
It can be seen that the points lie on a very convincing straight-line, which strongly suggests that
\begin{equation}
\frac{C_{\rm max}}{k\,N} \propto \ln L.
\end{equation}

\begin{figure}
\epsfysize=3in
\centerline{\epsffile{Chapter09/ts20.eps}}
\caption{\em The net magnetization, $M$, of a $20\times 20$ array of ferromagnetic atoms as a
function of the temperature, $T$, in the absence of an external magnetic
field. Monte-Carlo simulation.}
\end{figure}

Of course, for physical systems, $L\sim \sqrt{N_A} \sim 10^{12}$, where $N_A$ is Avogadro's
number. Hence, $C$ is effectively {\em singular} at the critical temperature
(since $\ln N_A \gg 1$), as sketched in 
Fig.~\ref{sketch}. This observation leads us to revise our definition of a second-order phase
transition. It turns out that actual discontinuities in the heat capacity almost
never occur. Instead, second-order phase transitions are
characterized by a {\em local quasi-singularity} in the heat capacity.

\begin{figure}
\epsfysize=3in
\centerline{\epsffile{Chapter09/cv20.eps}}
\caption{\em The heat capacity, $C$, of a  $20\times 20$ array of ferromagnetic atoms as a
function of the temperature, $T$, in the absence of an external magnetic
field. Monte-Carlo simulation. The solid curve shows the heat capacity calculated
from Eq.~(\ref{direct}), whereas  the dotted curve shows the heat capacity calculated
from Eq.~(\ref{indirect}).}\label{ff9}
\end{figure}

Recall, from Eq.~(\ref{indirect}), that the typical amplitude of energy fluctuations is proportional
to the square-root of the heat capacity ({\em i.e.}, $\sigma_E\propto \sqrt{C}$). It
follows that the amplitude of energy fluctuations becomes {\em extremely large} in the vicinity
of a second-order phase transition.

\begin{figure}
\epsfysize=3in
\centerline{\epsffile{Chapter09/ts40.eps}}
\caption{\em  The net magnetization, $M$, of a  $40\times 40$ array of ferromagnetic atoms as a
function of the temperature, $T$, in the absence of an external magnetic
field. Monte-Carlo simulation.}
\end{figure}

Now, the main difference between our mean field and Monte-Carlo calculations is the existence
of residual magnetization for $T>T_c$ in the latter case. Figures~\ref{image1}--\ref{image5} 
show the magnetization pattern of  a $40\times 40$ array of ferromagnetic atoms, in thermal
equilibrium and in the absence of an external magnetic field, calculated at various temperatures.
It can be seen that for $T=20\,T_0$ the pattern is essentially random. However,
for $T=5\,T_0$, small clumps appear in the pattern. For $T=3\,T_0$, the clumps
are somewhat bigger. For $T=2.32\,T_0$, which is just above the critical temperature,
the clumps are global in extent. Finally, for $T=1.8\,T_0$, which is a little
below the critical temperature, there is almost complete alignment of the atomic spins.

\begin{figure}
\epsfysize=3in
\centerline{\epsffile{Chapter09/cv40.eps}}
\caption{\em  The heat capacity, $C$, of a  $40\times 40$ array of ferromagnetic atoms as a
function of the temperature, $T$, in the absence of an external magnetic
field. Monte-Carlo simulation. The solid curve shows the heat capacity calculated
from Eq.~(\ref{direct}), whereas the dotted curve shows the heat capacity calculated
from Eq.~(\ref{indirect}). }\label{ff10}
\end{figure}

The problem with the mean field model is that it assumes that all atoms are situated
in identical environments. Hence, if the exchange effect is not sufficiently
large to cause global alignment of the atomic spins then there is no alignment at all.
What actually happens when the temperature exceeds the critical temperature
 is that global alignment disappears, but local
alignment ({\em i.e.}, clumping) remains. Clumps are only eliminated by thermal
fluctuations once the temperature is significantly greater than the
critical temperature. Atoms in the middle of the clumps
are situated in a different environment than atoms on the clump boundaries. Hence,
clumps cannot occur in the mean field model. 

\begin{figure}
\epsfysize=3in
\centerline{\epsffile{Chapter09/cvmax.eps}}
\caption{\em The peak value of the heat capacity (normalized by $N\,k$) versus the logarithm of
the array size for a two-dimensional array of ferromagnetic atoms in the absence
of an external magnetic field. Monte-Carlo simulation. }\label{cvmax}
\end{figure}

\begin{figure}
\epsfysize=2in
\centerline{\epsffile{Chapter09/sketch.eps}}
\caption{\em A sketch of the expected variation  of the heat capacity versus
the temperature for a physical two-dimensional ferromagnetic system. }\label{sketch}
\end{figure}

\begin{figure}
\epsfysize=2in
\centerline{\epsffile{Chapter09/image1.eps}}
\caption{\em  Magnetization pattern of a $40\times 40$ array of ferromagnetic atoms in thermal
equilibrium and in the absence of an external magnetic field. Monte-Carlo calculation with $T=20\,T_0$.
Black/white squares indicate atoms magnetized in plus/minus $z$-direction, respectively.}\label{image1}
\end{figure}

\begin{figure}
\epsfysize=2in
\centerline{\epsffile{Chapter09/image2.eps}}
\caption{\em   Magnetization pattern of a $40\times 40$ array of ferromagnetic atoms in thermal
equilibrium and in the absence of an external magnetic field. Monte-Carlo calculation with $T=5\,T_0$.
Black/white squares indicate atoms magnetized in plus/minus $z$-direction, respectively.}
\end{figure}

\begin{figure}
\epsfysize=2in
\centerline{\epsffile{Chapter09/image3.eps}}
\caption{\em  Magnetization pattern of a $40\times 40$ array of ferromagnetic atoms in thermal
equilibrium and in the absence of an external magnetic field. Monte-Carlo calculation with $T=3\,T_0$.
Black/white squares indicate atoms magnetized in plus/minus $z$-direction, respectively.}
\end{figure}

\begin{figure}
\epsfysize=2in
\centerline{\epsffile{Chapter09/image4.eps}}
\caption{\em    Magnetization pattern of a $40\times 40$ array of ferromagnetic atoms in thermal
equilibrium and in the absence of an external magnetic field. Monte-Carlo calculation with $T=2.32\,T_0$.
Black/white squares indicate atoms magnetized in plus/minus $z$-direction, respectively.}
\end{figure}

\begin{figure}
\epsfysize=2in
\centerline{\epsffile{Chapter09/image5.eps}}
\caption{\em   Magnetization pattern of a $40\times 40$ array of ferromagnetic atoms in thermal
equilibrium and in the absence of an external magnetic field. Monte-Carlo calculation with $T=1.8\,T_0$.
Black/white squares indicate atoms magnetized in plus/minus $z$-direction, respectively.}\label{image5}
\end{figure}